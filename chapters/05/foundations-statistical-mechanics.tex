\chapter{Theoretical foundations of statistical mechanics}

\section{Introduction}

	\subsection{Loschmidt's paradox}
	When studying Newton's equation in any formulation, inverting the arrow of time, it is always possible to trace back any trajectory and to obtain the solutions.
	This is intrinsic in the Newton's equations.
	In reality this is not the case: some of the processes cannot be traced back and are irreversible.
	This is	Loschmidts' paradox: the problem is that macroscopic systems are composed of a too large number of particles.
	And although in principle, the trajectory of each of them can be known, in the macroscopic process things are not obvious and a reasoning in term of probability has to be done.
	This is the objective of statistical mechanics: try to describe system with a very large number of degrees of freedom.

\section{Thermodynamics}

	\subsection{Thermodynamic system}
	A thermodynamic system is any macroscopic system.
	In general a macroscopic system has a number of particles similar to the Avogadro number.
	This is not the case for molecular simulations, but the system is close to the thermodynamic limits but it is not a macroscopic system.
	The system is not really a thermodynamic system.

		\subsubsection{Isolated systems}
		A system is defined as isolated whenever no energy or mass transfer can be seen across its boundary.
		Considering the system's boundaries in molecular simulations periodic boundaries conditions are used and the reason that they are used is because boundaries effects have to be avoided.
		The boundaries will be more important the smaller the system is.

	\subsection{Thermodynamic equilibrium}
	A thermodynamic system is in thermodynamic equilibrium if its thermodynamic state does not change in time.
	When dealing with biological system the equilibrium is never reached, but only stationary or steady-states.
	This means that the system is in a local equilibrium and will remain there most of the time.

	\subsection{Thermodynamic state}
	A thermodynamic state is specified in terms of macroscopic parameters that are measurable quantities like:

	\begin{multicols}{2}
		\begin{itemize}
			\item Pressure $P$.
			\item Volume $V$.
			\item Temperature $T$.
			\item Total mass $M$.
			\item Number of particles $N$.
		\end{itemize}
	\end{multicols}

	This is important because some procedures are needed to measure starting from the coordinates and the velocity of the system these quantities in a simulation.
	These are the parameters that are compared with the experimental results.

	\subsection{Equation of state}
	Whenever a thermodynamic system is being described an equation can be written and that equations establishes a relations between the measurables:

	$$g(N, P, V, T) = 0$$

	As an example the equation of state for the ideal gas is:

	$$PV-nRT=0$$

	The equation of state deals with thermodynamics state, which are assumed to be at equilibrium.
	So the equation of state describes only equilibrium states.

	\subsection{Thermodynamic transformations}
	A thermodynamic transformation is a change of thermodynamic state.
	At equilibrium the only way to change it is to change the external conditions, like changing pressure.
	These transformations can be:

	\begin{multicols}{2}
		\begin{itemize}
			\item Reversible: following equilibrium states and it is always possible to have the reverse transformation.
			\item Irreversible: non-equilibrium states are reached and when going back a different path is created.
				It is possible to reverse the process but the transformation is different from the forward one.
		\end{itemize}
	\end{multicols}

	\subsection{State function}
	A state function is defined as:

	$$f(n, P, V, T)$$

	It does not depend on the pathway of a transformation.
	And its change depends only on the initial and final states.
	There are many state functions like entropy or total energy.
	They are important because they can describe the effect of a transformation without knowing the nature of the transformation.
	The state functions itself depend on some parameters, which are the thermodynamics variables, which are not independent to each other.

	\subsection{Work}
	A reversible work performed on a system is defined as:

	$$dW_{rev} = -PdV + \mu dN$$

	So an infinitesimal amount of work come from a pressure contribution and the other is the contribution coming from the change of particles in the system.
	In this case the work is positive when the volume is decreasing, adding particles there is work performed on a system.
	$\mu$ is the chemical potential.

	\subsection{Heat}
	The heat added to the system is:

	$$dQ_{rev} = CdT$$

	Heat is assumed to be positive when it is added to the system and that it is added in a reversible manner.
	A change in heat correspond to a change in temperature if there is not a change transition.
	During a phase transition the temperature remains fixed but heat is still added.
	$C$ is the specific heat.

	\subsection{First law of thermodynamics}
	In any thermodynamic transformation if a system absorbs an amount of heat $\Delta Q$ and has an amount of work $\Delta W$ performed on it, its internal energy will change by an amount $\Delta E$ given by:

	$$\Delta E = \Delta Q + \Delta W$$

	This law is the conservation of energy: adding heat or performing work on the system energy is being provided.
	Where $E$ is a state function, although $Q$ and $W$ are not:

	$$\Delta E = \Delta Q_{rev} + \Delta W_{rev} = \Delta Q_{irrev} + \Delta W_{irrev}$$

	So the change in energy depend only on the initial and final state: it is always possible to compute the difference in energy for every transformation knowing only the initial and final state.

	\subsection{Second law of thermodynamics}
	There are two statements for the second law of thermodynamics which are completely equivalent.

		\subsubsection{Kelvin's formulation}
		There exists no thermodynamic transformation whose sole effect is to extract a quantity of heat from a high-temperature source and convert it entirely into work.

		\subsubsection{Clausius' formulation}
		There exists no thermodynamic transformation whose sole effect is to extract a quantity of heat from a cold source and deliver it to a hot source.

	\subsection{Entropy}
	The second law allow to define entropy.
	Entropy is a state function defined as:

	$$\Delta S = S_2-S_1 = \int_1^2\frac{dQ_{rev}}{T}$$

	The change in entropy depends only on the initial and the final state.
	It is a direct measure of the disorder of the system.
	There is no tool that can measure directly entropy.

	\subsection{Third law of thermodynamics}
	The third law of thermodynamics allow to find a reference state which can help to compute the absolute value of entropy.
	The reference state is the one at $0K$.
	The entropy of a system at the absolute zero of temperature is a universal constant, which can be taken to be zero.
	Finding a reversible transformation from $0K$ to any state an absolute value for any state and system can be found.

\section{The ensemble}
Defining thermodynamics from a macroscopic perspective the central concept is the ensemble.
A collection of systems described by the same set of microscopic interactions and sharing a common set of macroscopic properties is said to be an ensemble.
Only equilibrium ensembles will be considered.
In equilibrium ensembles the systems in them evolve in time, but average quantities remain the same.
So all particles move in time, but the average quantities of the macroscopic measurables remain the same.
Now considering a macroscopic observable $A$:

$$A = \frac{1}{Z}\sum\limits_{\lambda=1}^N\underbrace{a(x_\lambda)}_{\text{Phase space microscopic function}}\equiv\overbrace{\langle a\rangle}^{\text{Ensemble average}}$$

So any macroscopic observable is an average of the ensemble.
So during a simulation there are several copies of a system, then a simulation of each copy must be taken and the average will be the result.

	\subsection{Phase space volume}
	A microstate of a system is a point in the phase space:

	$$x_0 = (q_1(0), \dots, q_{3N}(0), p_1(0), \dots, p_{3N}(0))$$

	Assuming that this is the initial state of the system and its evolution according to Hamilton's equation will be observed.
	The phase space volume element $dx_0$ contains a collection of microstates surrounding $x_0$.
	Thinking in term of ensemble the difference in macroscopic quantities will be small and will contain a number of microstate that can or not be part of the ensemble.
	This volume element evolves according to Hamilton's equation into $dx_t$, another volume element:

	$$dx_t = J(x_t;x_0)dx_0$$

	Where:

	\begin{multicols}{2}
		\begin{itemize}
			\item $J(x_t;x_0) = det J$.
			\item $J_{kl} = \frac{\partial x_t^k}{\partial x_0^l}$ is the Jacobian.
		\end{itemize}
	\end{multicols}

	The time evolution of the phase space volume requires knowledge of the time evolution of the Jacobian.

		\subsubsection{Time evolution of the Jacobian}
		Consider the determinant of the Jacobian:

		$$det J = e^{Tr(\ln J)}$$

		This is the exponential of the trace of the logarithm of $J$ and this allow to take easier derivatives.
		The trace operator is the sum of the diagonal element of the matrix and it has the property to be always the same of a matrix: it is an invariant.
		In the case of the Jacobian:

		$$Tr J = \sum\limits_k J_{kk} = \sum\limits_k\lambda_k$$

		Where $\lambda_k$ are the eigenvalues.
		Then:

		$$e^{Tr(\ln J)} = e^{\sum\limits_k\ln\lambda_k} = \prod\limits_k\lambda_k$$

		Now the time evolution of the Jacobian:

		\begin{align*}
			\frac{d}{dt}J(x_t;x_0) &= \frac{d}{dt} det J = \frac{d}{dt}e^{Tr(\ln J)} = e^{Tr(\ln J)}Tr\biggl(\frac{dJ}{dt}J^{-1}\biggr) = \\
														 &= J(x_t;x_0)\sum\limits_{k, l}\biggl(\frac{d J_{kl}}{dt}J_{lk}^{-1}\biggr) = J(x_t;x_0)\sum\limits_{k,l}\biggl(\frac{\partial\dot{x}_t^k}{\partial x_0^l}\frac{\partial x_0^l}{\partial x_t^k}\biggr) =\\
														 &=j(x_t;x_0)\sum\limits_k\frac{\partial\dot{x}_t^k}{\partial x_t^k} = 0
		\end{align*}

		It is equal to zero because it is the compressibility of the system.
		So the time derivative of the quantity is equal to zero, so it means that the quantity itself is constant, so that the quantity, the determinant is equal to zero.

	\subsection{Liouville's  theorem}
	In particular:

	$$\frac{dJ(x_t;x_0)}{dt} = 0\Rightarrow J(x_t;x_0) = const$$

	However:

	$$J(x_0;x_0) = 1\Rightarrow J(x_t;x_0) = 1$$

	This is because a transformation into itself is an identity matrix, so it is always equal to $1$, so the determinant of the Jacobian is always equal to $1$.
	The phase space element $dx_0$ does not change in time and $dx_t = dx_0$.

	\subsection{Ensemble distribution function}
	Focussing  on a point $x$ in phase space and on a small volume around it.
	The ensemble distribution function $f(x,t)dx$ is the fraction of the total ensemble members contained in the phase space volume element $dx$ at time $t$.
	$f(x,t)$ verifies the typical properties of a probability density:

	$$f(x,t) \ge 0$$

	And integrating over the entirety of the phase space:

	$$\int f(x,t)dx = 1$$

	So there is a relation between the probability density and the ensemble distribution function.
	The objective of statistical mechanics is to find the shape of this function $f$.

	\subsection{Outward flux}
	Now the focus is how the ensemble distribution function evolves in time.
	The rate of decrease of ensemble members in a given volume $\Omega$ is:

	$$-\frac{d}{dt}\int_\Omega f(x_t, t)dx_t = -\int_\Omega dx_t\frac{\partial f(x,t)}{\partial t}$$

	So the integral over a volume element $\Omega$, counting the fraction of ensemble members in the volume, minus the time derivative of that is the rate of decrease.
	Assuming that $\Omega$ is not changing in time, $dx_t$ does not change because of Liouville's theorem.
	The only thing that might change is the ensemble distribution function, and it changes only if the ensemble distribution function has an explicit time dependence.
	The only way to change the number of the ensemble member is only if the ensemble distribution function has an explicit time dependence.
	There are no sink or sources of ensemble member and they change only if ensemble members pass through the surface $S$ of the volume $\Omega$.
	The outward flux of ensemble members leaving $\Omega$ is then

	$$\int_S\dot{x}_t\cdot\hat{n} f(x_t,t)$$

	Where $\dot{x}_t$ is the velocity in phase space with a dot product with a vector $\hat{n}$, a unit vector normal to the surface times the ensemble distribution function.
	With this quantity the divergence theorem can be applied:

	$$\int_S\dot{x}_t\cdot\hat{n}f(x_t,t) = \int_\Omega\nabla_{x_t}\cdot[\dot{x}_tf(x_t,t)]dx_t$$

	So the surface integral is translated in a volume integral taking the divergence of vector $\dot{x}_t f(x_t, f)$.

	\subsection{Liouville's equation}
	Writing down together the previous two results, the volume integral, the rate of decrease of ensemble element is equal to the flux:

	$$\int_\Omega\nabla_{x_t}\cdot[\dot{x}_tf(x_t, t)]dx_t = -\int_\Omega\frac{\partial f(x_t, t)}{\partial t}dx_t$$

	So the number inside the integral have to coincide:

	$$\int_\Omega\biggl\{\frac{\partial f(x_t, t)}{\partial t} + \nabla_{x_t}\cdot[\dot{x}_tf(x_t, t)]\biggr\}dx_t = 0\Rightarrow \frac{\partial f(x_t, t)}{\partial t} + \nabla_{x_t}\cdot[\dot{x}_tf(x_t, t)] = 0$$

	This implies that the argument of the integral has to be equal to $0$ because it is valid for any value of $\Omega$, obtaining a differential equation that will be satisfied for the ensemble distribution function.
	Writing down the equation more explicitly on the divergence:

	$$\frac{\partial f(x_t, t)}{\partial t} + (\nabla_{x_t}\cdot\dot{x}_t)f(x_t, t) + \dot{x}\cdot\nabla_{x_t}f(x_t, t) = 0$$

	Note that the divergence of the velocity is the compressibility, so that it is equal to zero, cancelling out the second element:

	$$\frac{\partial f(x_t, t)}{\partial t} + \dot{x}_t\cdot\nabla_{x_t}f(x_t, t) = 0\Rightarrow \frac{df(x_t, t)}{dt} = 0$$

	So that the total derivative of $f$ is equal to zero.
	This means that the ensemble distribution function won't change in time.

		\subsubsection{Consequence on averages}
		Considering the previous result:

		$$\frac{df(x_t, t)}{dt} = 0\Rightarrow f(x_t, t) = f(x_0, 0)$$

		And putting together Liouville's theorem and equation:

		$$f(x_t, t)dx_t = f(x_0, 0)dx_0$$

		So that there is a conserved fraction of ensemble members and averages can be performed at any time.
		So the result on averages is always the same because the ensemble members will be conserved during the trajectory.

		\subsubsection{A more elegant form}
		In a more elegant formalism the same formula can be written introducing $\eta$ the velocity, written down in term of the Hamiltonian

		$$\frac{\partial f(x,t)}{\partial t} + \dot{x}\cdot\nabla_x f(x,t) = \frac{\partial f(x,t)}{\partial f} + \eta(x,t)\cdot\nabla_{x}f(x,t) = 0$$

		So that the function corresponds to the Poisson's brackets:

		$$\frac{\partial f(x,t)}{\partial t} + \{f(x,t), \mathcal{H}(x,t)\} = 0$$

	\subsection{Equilibrium solutions}
	For a macroscopic observable $A$:

	$$A = \langle a(x)\rangle = \int a(x)f(x,t)dx$$

	This is an average over an ensemble of a microscopic coordinate.
	This can be written as the integral using the ensemble distribution function and that is the ensemble average.
	So to have $A$ constant in time $f$ cannot depend on time.
	An explicit time dependence of $t$ implies an explicit time dependence of $A$ hence at equilibrium it is required that:

	$$\frac{\partial f(x,t)}{\partial t} = 0\Rightarrow \{f(x,t), \mathcal{H}(x, t)\} = 0$$

	So ensemble distribution functions must satisfy this equation.
	So there is a general solution:

	$$f(x)\propto\mathcal{F}(\mathcal{H}(x))$$

	Where the ensemble distribution function, a probability density must be proportional to some function of the Hamiltonian.
	So that the solution can be written exactly:

	$$Z = \int dx\mathcal{F}(\mathcal{H}(x))\Rightarrow f(x) = \frac{1}{Z}\mathcal{F}(\mathcal{H}(x))$$

	So that the number $Z$ a function of the thermodynamics parameters is a partition function and $f$ can be written exactly.
	The integral is on the entire phase space.
	Most of statistical mechanics involves on computing the partition function, allowing to find the ensemble distribution function to find all the macroscopic observables allowing to compare with experimental results.
